import streamlit as st
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

# ==========================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ –ò –°–¢–ò–õ–ò
# ==========================================
st.set_page_config(page_title="Smart Ads.txt Validator", layout="wide", page_icon="üõ°Ô∏è")

st.markdown("""
    <style>
    .valid { color: #28a745; font-weight: bold; }
    .partial { color: #ffc107; font-weight: bold; }
    .error { color: #dc3545; font-weight: bold; }
    </style>
""", unsafe_allow_html=True)

st.title("üõ°Ô∏è Smart Ads.txt / App-ads.txt Validator")
st.markdown("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–ø–∏—Å–µ–π —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ (DIRECT/RESELLER) –∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.")

# ==========================================
# 2. –§–£–ù–ö–¶–ò–ò –õ–û–ì–ò–ö–ò (BACKEND)
# ==========================================

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∫–æ–ª–ª–µ–≥–∏)
LIVE_UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
session = requests.Session()
session.headers.update({
    'User-Agent': LIVE_UA,
    'Accept': 'text/plain,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
})

def fetch_file_content(domain, filename):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –¥–æ–º–µ–Ω–∞. –ü—Ä–æ–±—É–µ—Ç HTTPS, –∑–∞—Ç–µ–º HTTP.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, status_message, error_bool)
    """
    domain = domain.strip().replace("https://", "").replace("http://", "").split("/")[0]
    urls = [f"https://{domain}/{filename}", f"http://{domain}/{filename}"]
    
    for url in urls:
        try:
            response = session.get(url, timeout=10, allow_redirects=True)
            if response.status_code == 200:
                return response.text, "OK", False
            elif response.status_code == 403:
                # –ò–Ω–æ–≥–¥–∞ 403 –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –±–æ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, –Ω–æ —Ñ–∞–π–ª –µ—Å—Ç—å.
                # –ù–æ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ —ç—Ç–æ –æ—à–∏–±–∫–∞.
                continue
        except requests.exceptions.SSLError:
            try:
                # –ü—Ä–æ–±—É–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ SSL (–∫–∞–∫ –≤ —Å–∫—Ä–∏–ø—Ç–µ –∫–æ–ª–ª–µ–≥–∏)
                response = session.get(url, timeout=10, allow_redirects=True, verify=False)
                if response.status_code == 200:
                    return response.text, "OK (SSL warning)", False
            except:
                continue
        except Exception:
            continue
            
    return None, "File unreachable or 404", True

def parse_ads_file(content):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç ads.txt –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (#).
    """
    parsed_lines = []
    if not content:
        return parsed_lines

    for line in content.splitlines():
        # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean_line = line.split('#')[0].strip()
        if not clean_line:
            continue
        
        parts = [p.strip() for p in clean_line.split(',')]
        if len(parts) >= 3:
            parsed_lines.append({
                'domain': parts[0].lower(),
                'id': parts[1].lower(), # ID –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å–≤–µ—Ä–∫–∏
                'type': parts[2].upper(),
                # Authority ID (4-–π –ø–∞—Ä–∞–º–µ—Ç—Ä) –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω, –∑–¥–µ—Å—å –Ω–µ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –º–∞—Ç—á–∏–Ω–≥–∞
            })
    return parsed_lines

def parse_reference_line(line):
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—ç—Ç–∞–ª–æ–Ω)
    –ü—Ä–∏–º–µ—Ä: google.com, pub-8309773808661346, RESELLER, f08c47fec0942fa0
    """
    parts = [p.strip() for p in line.split(',')]
    if len(parts) < 2:
        return None
    
    return {
        'domain': parts[0].lower(),
        'id': parts[1].lower(),
        'type': parts[2].upper() if len(parts) > 2 else None,
        'original': line
    }

def validate_domain(target_domain, filename, references):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–≤–µ—Ä–∫–∏
    """
    content, status_msg, is_error = fetch_file_content(target_domain, filename)
    
    results = []
    
    # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å–∫–∞—á–∞–ª—Å—è
    if is_error:
        for ref in references:
            results.append({
                "URL": target_domain,
                "File": filename,
                "Result": "Error",
                "Details": status_msg,
                "Reference": ref['original']
            })
        return results

    # –ü–∞—Ä—Å–∏–º —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
    file_records = parse_ads_file(content)
    
    for ref in references:
        ref_domain = ref['domain']
        ref_id = ref['id']
        ref_type = ref['type'] # –ú–æ–∂–µ—Ç –±—ã—Ç—å None, –µ—Å–ª–∏ —é–∑–µ—Ä –Ω–µ —É–∫–∞–∑–∞–ª
        
        match_found = False
        match_status = "Not found"
        details = "No matching Domain+ID pair found"
        
        # –õ–û–ì–ò–ö–ê –ü–û–ò–°–ö–ê (Priority Match)
        for record in file_records:
            # 1. –°–≤–µ—Ä—è–µ–º –î–æ–º–µ–Ω –∏ ID
            if record['domain'] == ref_domain and record['id'] == ref_id:
                # –ü–∞—Ä–∞ –Ω–∞–π–¥–µ–Ω–∞! –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø.
                
                # –ï—Å–ª–∏ —Ç–∏–ø –≤ —ç—Ç–∞–ª–æ–Ω–µ –Ω–µ —É–∫–∞–∑–∞–Ω, —Å—á–∏—Ç–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ ID
                if not ref_type:
                    match_status = "Valid"
                    details = "Matched by Domain + ID (Type not specified)"
                    match_found = True
                    break
                
                # –ï—Å–ª–∏ —Ç–∏–ø —É–∫–∞–∑–∞–Ω, —Å–≤–µ—Ä—è–µ–º
                if record['type'] == ref_type:
                    match_status = "Valid"
                    details = "Full match"
                    match_found = True
                    break
                else:
                    match_status = "Partially matched"
                    details = f"Type mismatch: found {record['type']}, expected {ref_type}"
                    match_found = True
                    # –ù–µ –¥–µ–ª–∞–µ–º break, –≤–¥—Ä—É–≥ –¥–∞–ª—å—à–µ –≤ —Ñ–∞–π–ª–µ –µ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –Ω—É–∂–Ω—ã–º —Ç–∏–ø–æ–º?
                    # –ù–æ –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–º –ø–æ–ª–Ω—É—é, –æ—Å—Ç–∞–≤–∏–º Partial.
        
        results.append({
            "URL": target_domain,
            "File": filename,
            "Result": match_status,
            "Details": details,
            "Reference": ref['original']
        })
        
    return results

# ==========================================
# 3. –ò–ù–¢–ï–†–§–ï–ô–° (UI)
# ==========================================

# --- –û–∫–Ω–æ 1: –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ ---
col1, col2 = st.columns([1, 3])
with col1:
    st.subheader("1. Settings")
    file_type = st.radio(
        "Select file to check:",
        ("ads.txt", "app-ads.txt"),
        index=0 # –ü–æ –¥–µ—Ñ–æ–ª—Ç—É ads.txt, –Ω–æ –≤—ã–±–æ—Ä –≤–∏–¥–µ–Ω —è–≤–Ω–æ
    )
    
    threads = st.slider("Threads (Speed)", min_value=1, max_value=50, value=20)

# --- –û–∫–Ω–æ 2: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ---
with col2:
    st.subheader("2. Input Data")
    
    tab_targets, tab_refs = st.tabs(["üåê Target Websites", "üìù Reference Lines (Rules)"])
    
    with tab_targets:
        target_input = st.text_area(
            "Sites to check (URLs or Domains)", 
            height=150,
            placeholder="example.com\nmygame.site\nhttps://news-portal.org"
        )
        
    with tab_refs:
        ref_input = st.text_area(
            "Reference Lines (What to look for)", 
            height=150, 
            placeholder="google.com, pub-8309773808661346, RESELLER\nonetag.com, 5d0d72448d8bfb0, DIRECT"
        )
        st.caption("Format: `domain, id, type` (comma separated)")

# --- –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ ---
start_btn = st.button("üöÄ Start Validation", type="primary", use_container_width=True)

# ==========================================
# 4. –û–ë–†–ê–ë–û–¢–ö–ê –ò –í–´–í–û–î (EXECUTION)
# ==========================================

if start_btn:
    if not target_input or not ref_input:
        st.error("Please provide both Target Websites and Reference Lines.")
    else:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        targets = [t.strip() for t in target_input.splitlines() if t.strip()]
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        references = []
        raw_refs = [r.strip() for r in ref_input.splitlines() if r.strip()]
        for r in raw_refs:
            parsed = parse_reference_line(r)
            if parsed:
                references.append(parsed)
            else:
                st.warning(f"Skipping invalid reference format: {r}")

        if not references:
            st.stop()

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        all_results = []
        
        # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –∑–∞–ø—É—Å–∫
        with ThreadPoolExecutor(max_workers=threads) as executor:
            future_to_url = {
                executor.submit(validate_domain, url, file_type, references): url 
                for url in targets
            }
            
            for i, future in enumerate(as_completed(future_to_url)):
                url = future_to_url[future]
                try:
                    data = future.result()
                    all_results.extend(data)
                except Exception as e:
                    # –õ–æ–≤–∏–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –ø–æ—Ç–æ–∫–∞
                    all_results.append({
                        "URL": url, "File": file_type, 
                        "Result": "System Error", "Details": str(e), 
                        "Reference": "-"
                    })
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                progress = (i + 1) / len(targets)
                progress_bar.progress(progress)
                status_text.text(f"Processed {i + 1}/{len(targets)} sites")

        progress_bar.empty()
        status_text.empty()
        
        # --- –û–∫–Ω–æ 3: –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
        st.divider()
        st.subheader("3. Results")
        
        df = pd.DataFrame(all_results)
        
        # –£–ø–æ—Ä—è–¥–æ—á–∏–º –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª
        # 1. URL, 2. File, 3. Result, 4. Details + (Reference –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏)
        cols_order = ["URL", "File", "Result", "Details", "Reference"]
        df = df[cols_order]

        # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã (—Ä–∞—Å–∫—Ä–∞—Å–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤)
        def color_status(val):
            if val == "Valid":
                return 'background-color: #d4edda; color: #155724' # Green
            elif val == "Partially matched":
                return 'background-color: #fff3cd; color: #856404' # Yellow
            elif val == "Not found":
                return 'background-color: #f8d7da; color: #721c24' # Red
            return ''

        st.dataframe(
            df.style.map(color_status, subset=['Result']),
            use_container_width=True,
            height=600
        )
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üíæ Download Report (CSV)",
            data=csv,
            file_name=f"ads_txt_validation_{file_type}.csv",
            mime="text/csv",
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        st.write("---")
        col_stat1, col_stat2, col_stat3 = st.columns(3)
        col_stat1.metric("Valid", len(df[df['Result'] == 'Valid']))
        col_stat2.metric("Partial Matches", len(df[df['Result'] == 'Partially matched']))
        col_stat3.metric("Not Found / Errors", len(df[df['Result'].isin(['Not found', 'Error'])]))
